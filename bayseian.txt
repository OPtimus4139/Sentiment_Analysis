import optuna
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from keras.datasets import imdb
from keras.preprocessing import sequence

# Load and prepare data
max_features = 5000  # Vocabulary size
max_len = 100  # Maximum sequence length
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=max_len)
x_test = sequence.pad_sequences(x_test, maxlen=max_len)

# Define objective function for Optuna
def objective(trial):
    model = Sequential()
    # Sample hyperparameters
    lstm_units = trial.suggest_int("lstm_units", 50, 200)
    dropout_rate = trial.suggest_float("dropout_rate", 0.1, 0.5)
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True)
    
    # Build model
    model.add(LSTM(units=lstm_units, input_shape=(max_len, 1)))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation="sigmoid"))
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss="binary_crossentropy", metrics=["accuracy"])
    
    # Train and evaluate
    model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0, validation_split=0.2)
    score = model.evaluate(x_test, y_test, verbose=0)
    return score[0]  # Return validation loss

# Run Bayesian Optimization
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=50)

# Best hyperparameters
print("Best hyperparameters: ", study.best_params)
